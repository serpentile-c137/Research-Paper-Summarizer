{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\ganesh\\miniconda3\\envs\\llms\\lib\\site-packages (4.12.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "import glob\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# Upgrade typing_extensions to fix ImportError\n",
    "%pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r'/Users/shardulgore/Documents/Projects/Research-Paper-Summarizer/papers/Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf'\n",
    "file_path = r'../papers/Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(file_path, MODEL):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5) \n",
    "    chain = load_summarize_chain(llm, chain_type='map_reduce')\n",
    "    summary = chain.invoke(docs)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper investigates the explainability of MedCLIP, a vision-language model used for medical image analysis (specifically chest X-rays). It analyzes existing XAI methods, finding them insufficient for VLMs due to their complex text-image interaction and the generation of false positives. The authors propose a novel XAI approach that generates more accurate feature activation maps by integrating image and text embeddings. This method aims to improve the trustworthiness and wider adoption of VLMs in healthcare by providing better insights into model predictions, and is applicable to other VLMs. The research builds on the MIMIC-CXR dataset and leverages various AI tools and resources.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(file_path):\n",
    "\tsumm = summarize(file_path, MODEL)\n",
    "\tprint(summ['output_text'])\n",
    "else:\n",
    "\tprint(f\"File path {file_path} is not a valid file or url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT = \"Write a detailed summary of the methodology used in following paper. Give output in markdown format.\"\n",
    "vectorDB_path = 'faiss_store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load_and_split()\n",
    "prompt_template = CUSTOM_PROMPT + \"\"\"\n",
    "Answer the following question based only on the provided context, do not use any external information. Always give a detailed answer in a language such that the answer can be used in summary:\n",
    "\n",
    "<context>\n",
    "{text}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "vector_store.save_local(vectorDB_path)\n",
    "\n",
    "vectorstore = FAISS.load_local(vectorDB_path, embeddings, allow_dangerous_deserialization=True)\n",
    "# chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"question\": PROMPT.template, \"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The methodology aims to enhance the explainability of Vision-Language Models (VLMs), specifically MedCLIP, by applying Explainable AI (XAI) methods to the embedding space of the VLM. The approach involves the following steps:\n",
      "\n",
      "1.  **XAI Method Selection:** Four XAI methods are selected: Gradient backpropagation, Occlusion method, Integrated-Gradients, and Grad-Shapley. These methods are chosen to represent a diverse set of techniques.\n",
      "2.  **Application of XAI Methods:** The selected XAI methods are applied to the MedCLIP model.\n",
      "3.  **Proposed Approach:**\n",
      "    *   **Image Embedding Analysis:** An XAI method (Mxai) is applied to the vision encoder (Venc) of MedCLIP to generate an explainability map (Fi\\_map) for each embedding dimension (i). The output is M distinct maps, where M is the dimension of the embedding space. `Fi_map = Mxai(model = Venc, target= i)`.\n",
      "    *   **Text Embedding Generation:** A text input (Xtxt) is encoded using the text encoder (Tenc) to generate text embeddings (Tp ∈ R1xM). These embeddings are scaled by the learned temperature parameter of the VLM.\n",
      "    *   **Fusion and Final Explainability Map:** A dot product is calculated between the image explainability maps and the text embeddings. This results in a weighted average (Fout\\_map) of the image explainability maps, where the weights are determined by the similarity between the text and image embeddings. `Fout_map = Tp · Fall_map` where Fall\\_map ∈ RMxAxA is a list containing M generated feature maps. The final output is a single explainability map that highlights the image pixels that most influence the model's prediction, considering the given text input.\n",
      "4.  **Text Input:** Both class labels and text prompts (sentences) are used as text input to the model. A set of 10 prompts was developed for each class label, encompassing information about the specific pathology, its location, intensity, and variations in sub-types.\n",
      "5.  **Dataset:** The MIMIC-CXR dataset, a large CXR dataset with free-text radiology reports, is used. A subset of 2000 randomly selected samples along with the class labels is incorporated for the analysis.\n",
      "6.  **Implementation:** Experiments are performed on a single Nvidia Quadro RTX 6000 GPU with 24GB of memory. The MedCLIP model is implemented using the PyTorch library, and the Captum library is used for off-the-shelf XAI methods.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"question\": prompt_template}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a detailed summary of the methodology used in the paper, formatted in Markdown:\n",
      "\n",
      "**Methodology Summary**\n",
      "\n",
      "The paper's methodology centers on applying Explainable AI (XAI) techniques to analyze the inner workings of Vision-Language Models (VLMs), specifically focusing on MedCLIP for chest X-ray (CXR) classification. The approach involves the following key steps:\n",
      "\n",
      "1.  **XAI Method Selection:** The study employs four XAI methods:\n",
      "    *   **Gradient Backpropagation (GB):** Identifies important features by assigning importance scores based on their contribution to model predictions.\n",
      "    *   **Occlusion Method:** Analyzes region importance by occluding parts of the image and observing changes in network activation and model predictions.\n",
      "    *   **Integrated-Gradients (IG):** Quantifies pixel importance by interpolating a baseline image to the actual input and integrating the gradients.\n",
      "    *   **Grad-Shapley (GS):** Combines gradients with Shapley values to quantify each feature's contribution to the final prediction.\n",
      "\n",
      "2.  **Application to MedCLIP:** The selected XAI methods are applied to MedCLIP to generate explainability maps for a given image.\n",
      "\n",
      "3.  **MedCLIP Overview:** The paper provides a brief overview of MedCLIP's architecture:\n",
      "    *   A 224x224 input image (Ximg) is passed through a vision encoder (Venc – SwinTransformer) to produce image embeddings (I ∈ RD).\n",
      "    *   These embeddings are projected into a lower-dimensional vector Ip ∈ RM via a projection head (Pv).\n",
      "    *   The input text (Xtxt) is tokenized and encoded using a text encoder (Tenc – BioClinicalBERT) to produce a vector (T ∈ RE).\n",
      "    *   The resultant vector is projected to Tp ∈ RM using the text projection head (Pt).\n",
      "    *   Text and image embeddings (Ip and Tp) are normalized before calculating the dot product (Mdot) in a contrastive manner.\n",
      "    *   The final output logit (L) is calculated using the dot product (Mdot) and a learnable temperature parameter (τ).\n",
      "\n",
      "4.  **Proposed Approach:** The proposed approach focuses on applying XAI methods to the embedding space of the VLM, rather than directly to the final output. This involves:\n",
      "    *   **Image Embedding Explanation:** An XAI method (Mxai) is applied to the image embeddings generated by the vision encoder (Venc) of MedCLIP. This generates an explainability map (Fi map) for each embedding dimension (i). The maps highlight the important image pixels for each specific image embedding dimension. This results in M distinct maps, one for each embedding dimension.\n",
      "        *   Fi map = Mxai(model = Venc, target= i)\n",
      "        *   Where 'i' is the index of the image embedding (Ip ∈ R1x512).\n",
      "    *   **Text Embedding Generation:** A text input (Xtxt) is selected and encoded through the text encoder (Tend) to generate text embeddings (Tp ∈ R1xM). These generated embeddings are then scaled by the learned temperature parameter of the VLM.\n",
      "\n",
      "5.  **Explainability Maps Generation:** The approach generates feature activation maps to visualize how the VLM processes image and text inputs, using both text prompts (sentences describing lesions) and class labels as text inputs.\n",
      "\n",
      "6.  **Text Input Variation:** The method investigates the influence of different text inputs (sentences and class labels) on the VLM's focus.\n",
      "\n",
      "7.  **Evaluation with Chest X-ray Images:** The method is evaluated using chest X-ray (CXR) images and the resulting maps were analyzed.\n",
      "\n",
      "8.  **Comparison to Conventional XAI:** The proposed method is contrasted with conventional Explainable AI (XAI) methods, aiming to accurately highlight important image areas.\n",
      "\n",
      "9.  **Image and Text Embedding Fusion:** The method is adaptable to other VLMs by following the image and text embedding fusion approach used in that specific model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
