{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\ganesh\\miniconda3\\envs\\llms\\lib\\site-packages (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "import glob\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# Upgrade typing_extensions to fix ImportError\n",
    "%pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'E:\\Shardul\\Research-Paper-Summarizer\\papers\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(file_path, MODEL):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5) \n",
    "    chain = load_summarize_chain(llm, chain_type='map_reduce')\n",
    "    summary = chain.invoke(docs)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 0}, page_content='ENVISIONING MEDCLIP: A DEEP DIVE INTO EXPLAINABILITY FOR MEDICAL\\nVISION-LANGUAGE MODELS\\nAnees Ur Rehman Hashmi1, Dwarikanath Mahapatra2, Mohammad Yaqub1\\n1Mohamed bin Zayed University of Artificial Intelligence,\\n2Inception Institute of Artificial Intelligence\\nAbu Dhabi, UAE\\nABSTRACT\\nExplaining Deep Learning models is becoming increasingly\\nimportant in the face of daily emerging multimodal models,\\nparticularly in safety-critical domains like medical imaging.\\nHowever, the lack of detailed investigations into the perfor-\\nmance of explainability methods on these models is widening\\nthe gap between their development and safe deployment. In\\nthis work, we analyze the performance of various explainable\\nAI methods on a vision-language model, MedCLIP, to demys-\\ntify its inner workings. We also provide a simple method-\\nology to overcome the shortcomings of these methods. Our\\nwork offers a different new perspective on the explainability\\nof a recent well-known VLM in the medical domain and our\\nassessment method is generalizable to other current and pos-\\nsible future VLMs.\\nIndex Terms— Explainable AI, Vision-Language Mod-\\nels, Multimodal Models\\n1. INTRODUCTION\\nDeep Learning (DL) models provide a remarkable perfor-\\nmance on many tasks, however, they often work as a black\\nbox and their internal working mechanism is hidden from the\\nend-user [1]. This induces skepticism and a lack of trust in\\nthese models, particularly for cases like clinical diagnosis.\\nThereby hindering the real-life deployment and adaptation of\\nDL models in critical domains like healthcare.\\nExplainable AI (XAI) [2] offers a remedy to this problem\\nby providing insights into the inner workings of DL models. It\\nenables the end-users to see and understand the rationale be-\\nhind models’ prediction, enhancing fairness and confidence\\nin DL models. In recent years, a number of XAI methods\\nhave been introduced to demystify these black-box behavior\\nin DL. These methods differ from each other mainly based on\\nthe information they offer, their applicability (model specific\\nor agnostic), and the scope of their explanation [2]. Further-\\nmore, XAI methods can be categorized as intrinsic or post-\\nhoc depending on the provided interpretation. Intrinsic or\\nmodel-based methods rely on the structure of the model it-\\nself and hence are limited to models like linear regression.\\nWhereas post-hoc methods are applied to more complex mod-\\nels that are often difficult to explain. Examples of post-hoc\\nmethods include gradient backpropagation [3], layer-wise rel-\\nevance propagation [4], and class activation mapping (CAM)\\n[5].\\nMultimodal learning has shown great success in combin-\\ning the information from various modalities to increase the\\nperformance of deep learning models [6, 7]. Particularly,\\nvision-language models (VLM) utilize both visual and tex-\\ntual information to learn a meaningful representation [8, 9]\\nand to answer complex questions related to the associated\\ndata [10, 11]. These models bring AI a step closer to the\\nhuman-like working paradigm where AI models can process\\nmultiple types of inputs simultaneously to perform a task. It\\nalso enables the utilization of large-scale pre-trained vision\\nand language models for downstream tasks, which reduces\\nthe cost and hustle of training models from scratch.\\nVLMs have also demonstrated significant utility in med-\\nical applications, as evidenced by studies such as [12, 13,\\n14]. Moreover, the medical domain contains the paired scans-\\nreports that can be utilized to train VLMs for enhanced per-\\nformance in downstream tasks like classification, segmenta-\\ntion, and image generation [15, 16, 14]. A recent VLM, Med-\\nCLIP [14] efficiently trained the CLIP-like [17] model using a\\nmodified objective for X-Ray image-report dataset. However,\\ndespite being very useful, the complexity of VLMs makes\\nthem less plausible causing obstruction to their wide applica-\\ntions. Explaining these VLMs is of high importance but also'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 0}, page_content='despite being very useful, the complexity of VLMs makes\\nthem less plausible causing obstruction to their wide applica-\\ntions. Explaining these VLMs is of high importance but also\\nchallenging as typical XAI methods work with one modal-\\nity i.e. text or image alone [18], which can be misleading as\\nVLMs heavily rely on the interaction between text and image\\nfeatures. Furthermore, it is unclear whether single-modality-\\nbased explainability methods can explain VLMs.\\nTo address the problem of explainability in VLMs, we an-\\nalyze the effectiveness of different existing XAI methods to\\nexplain a recently introduced VLM, MedCLIP [14]. Further-\\nmore, we propose a simple and effective method to overcome\\nthe shortcomings of these methods by combining the XAI\\nmethods with the text and image interaction in VLM. This\\nincreases the plausibility as well as provides a framework to\\n2024 IEEE International Symposium on Biomedical Imaging (ISBI) | 979-8-3503-1333-8/24/$31.00 ©2024 IEEE | DOI: 10.1109/ISBI56570.2024.10635843\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 04,2025 at 05:54:21 UTC from IEEE Xplore.  Restrictions apply.'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 1}, page_content='understand what the VLM actually looks for when making a\\ncertain prediction.\\n2. METHODOLOGY\\nWe selected a diverse set of XAI methods for our analysis. A\\nbrief overview of these methods is as follows:\\n• Gradient backpropagation (GB) is a saliency-map-\\nbased method that backpropagates the gradients to find\\nthe features of high importance [19]. It assigns each input\\nfeature an importance score which indicates that feature’s\\ncontribution to the model prediction.\\n• Occlusion method is used for analyzing the importance\\nof each region in the input. It partly occludes an image\\nand observes the change in the network activation and the\\nmodel’s prediction [20]. The input regions that bring the\\nmost change are considered to be the most important.\\n• Integrated-Gradients (IG) interpolates a baseline image\\nto the actual input image to quantify the importance of\\neach pixel. The baseline image can be random noise or\\npure white pixels. This is an efficient method that inte-\\ngrates the gradients at different levels [21].\\n• Grad-Shapley (GS) combines the gradients of the model\\npredictions with respect to the input pixels and combines\\nthem with the Shapley value [22].\\nWe selected these four XAI methods for several reasons.\\nFirst, they represent a diverse range of techniques that offer\\ndistinct insights into the inner workings of the DL model.\\nGB helps us understand feature importance through the gra-\\ndient flow in the model. Occlusion provides insights into the\\nmodel’s sensitivity to different image regions by systemati-\\ncally occluding parts of the input. IG method offers a way to\\nattribute predictions to specific features by tracking their in-\\nfluence across input variations. GS method, on the other hand,\\nenables us to quantify the contribution of each feature’s inter-\\naction to the final prediction. Moreover, these methods are\\nwell-established and widely recognized in the field of XAI.\\nThey have been applied successfully across various domains,\\nmaking them suitable choices for our specific use case.\\nFor our analysis, we first applied these methods to Med-\\nCLIP in order to analyze their performance through explain-\\nability maps for a given image. This is followed by using\\nour proposed method to acquire the explainability maps and\\nanalyze the difference in interpretability.\\n2.1. MedCLIP\\nMedCLIP is a recently introduced powerful VLM for chest X-\\nray (CXR) classification. It combines a BioClinicalBERT 1-\\nbased text-encoder backbone and a SwinTransformer [23]\\nvision-encoder, pre-trained on the ImageNet dataset [24].\\nThis BERT model has been pre-trained on the MIMIC-III\\ndataset [25] containing the electronic health records from\\nICU patients. These transformer-based large models possess\\n1https://huggingface.co/emilyalsentzer/Bio ClinicalBERT\\nthe ability to efficiently learn complex features from the input\\ndata.\\nFirst, a 224x224 input image (X img) is passed through\\nthe vision encoder (Venc) to produce image embeddings (I ∈\\nRD). These embeddings are then further projected into a\\nlower-dimensional vector Ip ∈ RM via a projection head de-\\nnoted as Pv.\\nI = Venc(Ximg) (1)\\nIp = Pv(I) (2)\\nSimilarly, the input text (Xtxt) is tokenized and then encoded\\nusing the text encoder Tenc. The resultant vector (T ∈ RE) is\\nsubsequently projected to Tp ∈ RM using the text projection\\nhead Pt, to match the vision and text embeddings dimensions\\ndenoted as M (M=512 in MedCLIP).\\nT = Tenc(Xtxt) (3)\\nTp = Pt(T) (4)\\nThese text and image embeddings (Ip and Tp) are normalized\\nbefore calculating the dot product Mdot in a contrastive man-\\nner.\\nMdot = Ip · Tp (5)\\nL = Mdot ∗ τ (6)\\nHere L represents the final output logit, indicating the similar-\\nity between the input image and the text andτ is the learnable\\ntemperature parameter.\\n2.2. Proposed Approach\\nOur proposed approach is based on the idea of applying the\\nselected XAI methods to the embedding space of the VLM,\\ninstead of the final output of the model. This is motivated by'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 1}, page_content='Our proposed approach is based on the idea of applying the\\nselected XAI methods to the embedding space of the VLM,\\ninstead of the final output of the model. This is motivated by\\nthe fact that VLMs process each input separately before fus-\\ning them to generate the final output (see MedCLIP section)\\nand each of the encoders can be treated as separate models.\\nHence applying the existing XAI methods for each of the in-\\nput modalities and then combining it with the other modality\\ncan lead to better explainability. The proposed method has\\nthree main steps:\\n1. Firstly, an XAI method of choice Mxai is applied to the\\nimage embeddings generated by the vision-encoder of\\nVenc of the MedCLIP to generate an explainability map\\nFi\\nmap ∈ RAxA for each embedding dimension i. This\\nyields M distinct maps, each highlighting the input im-\\nage pixels important for one specific image embedding\\ndimension.\\nFi\\nmap = Mxai(model = Venc, target= i)\\nwhere i is the index of the image embeddingIp ∈ R1x512.\\n2. In the second step, a text input Xtxt is selected and en-\\ncoded through the text encoder Tend to generate the em-\\nbeddings Tp ∈ R1xM . These generated embeddings are\\nscaled by the learned temperature parameter of the VLM.\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 04,2025 at 05:54:21 UTC from IEEE Xplore.  Restrictions apply.'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 2}, page_content='Fig. 1. Class-specific feature maps generated for the prompt-classifier. These maps do not provide any significant class-specific\\ninformation and therefore are not suitable for explaining VLM like MedCLIP.\\n3. In the final step, a dot product between the image explain-\\nability maps generated in step 1 and text embeddings is\\ncalculated to get a RAxA weighted average of these maps\\nFout\\nmap. This step calculates the similarity of each text em-\\nbedding to the corresponding image embedding as well\\nas quantifies their contribution to the final output, thereby\\nproviding a measure of how important each explainability\\nmap is for the models’ prediction.\\nFout\\nmap = Tp · Fall\\nmap\\nwhere Fall\\nmap ∈ RMxAxA is a list containing M generated\\nfeature maps.\\nThis results in a single explainability map that highlights\\nthe specific image pixels influencing the model’s confidence\\nwith regard to the given input. It is important to note that\\nthis method is very efficient as the second and third steps can\\nbe repeated for different prompt embeddings once the com-\\nplete set of feature maps for an image is obtained, obviating\\nthe need for repeating step 1 for each input text. We exper-\\nimented with both class labels as well as text prompts (sen-\\ntences) as text input to the model. A set of 10 prompts was\\ndeveloped for each class label. Each generated text prompt\\nencompasses information about the specific pathology, its lo-\\ncation, intensity, and variations in sub-types (e.g. ”mild linear\\natelectasis at the mid lung zone”). This comparison between\\ntext prompts and class labels will help us visualize the effect\\nof different input types on the VLM.\\n2.2.1. Dataset\\nWe used the MIMIC-CXR [26] dataset which is a large CXR\\ndataset with free-text radiology reports collected from the\\nBeth Israel Deaconess Medical Center in Boston, MA. This\\ndataset contains approximately 377,110 CXR-report pairs\\nwith 14 classes (13 pathologies and 1 healthy class). We in-\\ncorporated a subset of 2000 randomly selected samples along\\nwith the class labels for our analysis.\\n2.2.2. Implementation Details\\nWe performed all experiments on a single Nvidia Quadro\\nRTX 6000 GPU with 24GB of memory. The MedCLIP model\\nwas implemented using the PyTorch library [27], while we\\nused the Captum library [28] for off-the-shelf XAI methods.\\n3. RESULTS\\nFigure 1 shows the class-wise explainability maps generated\\nusing the four selected XAI methods. It’s worth noting that\\nthese explainability maps exhibit a remarkable degree of sim-\\nilarity despite having significantly different inputs. What’s\\nmore, they collectively assign a substantial portion of the im-\\nage as important, which results in a high rate of false posi-\\ntives. This consistent high false-positive behavior is observed\\nacross all four methods. In fact, these methods often highlight\\npixels outside the human body in chest X-rays as equally cru-\\ncial for the final prediction. Furthermore, it is evident that the\\nclass labels do not exert a discernible influence on the final\\noutput. These results fail to align with the established medical\\ndiagnostic methodologies, which are typically lesion-specific,\\nas different regions of a chest X-ray are critical for different\\ndiagnoses. This discrepancy further underscores the limita-\\ntions of conventional XAI methods in effectively elucidating\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 04,2025 at 05:54:21 UTC from IEEE Xplore.  Restrictions apply.'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 3}, page_content='Fig. 2. Feature activation maps were generated using the proposed methods. These activation maps are focused and clearly\\nexplain MedCLIP. The top part shows show the VLM is able to focus on specific areas of the input image based on the input\\nprompts. Whereas, the lower part shows the activation maps for the used models when provided with the class labels as text\\ninputs. These maps comprehensively explain the difference in the performance of MedCLIP based on the type of text input\\nprovided.\\nthe mechanisms underlying the MedCLIP model. Our pro-\\nposed approach produces the feature maps depicted in Figure\\n2. We generated explainability maps using both text prompts\\n(sentences) and class labels to investigate the influence of dif-\\nferent text inputs. To begin, we applied our method to Med-\\nCLIP, utilizing images and text prompts describing various\\nlesions. As depicted in the top section of Figure 2, our ap-\\nproach stands in contrast to conventional XAI methods by\\navoiding false positives and accurately highlighting the most\\nimportant images. The highlighted pixel locations closely\\nalign with established clinical diagnostic procedures for the\\nspecified pathology. Additionally, our method effectively il-\\nlustrates how MedCLIP’s focus shifts based on the input text\\nprompt, providing strong evidence of this VLMs’ capacity to\\ncomprehend text and identify relevant image pixels. It is also\\nevident this method is capable of delivering nuanced, compre-\\nhensive, and meaningful insights into the model’s operation.\\nSecondly, we employed class labels in conjunction with chest\\nX-ray (CXR) images to evaluate our method, as shown in the\\nbottom section of Figure 2. The highlighted image pixels are\\nlocalized and exhibit variations across different class labels.\\n4. DISCUSSION AND CONCLUSION\\nIn this paper, we analyzed the usefulness of existing XAI\\nmethods for VLMs and provided a simple and highly effec-\\ntive method to overcome their shortcomings for the multi-\\nmodal models. Through our work, we demonstrate the ef-\\nficacy of our proposed approach in comprehensively explain-\\ning the functioning of the MedCLIP VLM, a feat that the con-\\nventional approach fails to achieve. Moreover, our approach\\nshows the combined effect of both input modalities on the\\nVLM prediction which can be of high importance in VLMs.\\nFurthermore, the explainability maps can help us understand\\nthe discrepancy in the performance of DL models. We vi-\\nsualize the difference in activation maps for two different text\\ninput forms in order to understand the effect of different types\\nof text inputs. One major benefit of this work is the flexibility\\nto use any off-the-shelf XAI method for VLMs, making the\\nmethod versatile. Furthermore, it can also be adapted to other\\nVLMs by following the image and text embedding fusion ap-\\nproach used in that specific model.\\nIn conclusion, pretrained VLMs like MedCLIP have enor-\\nmous potential to be used for downstream tasks without fine-\\ntuning. However, it is very important to make AI more trust-\\nworthy by making these models explainable to the end user.\\nFurther research is required to design new VLM-specific XAI\\nmethods or frameworks for practical use of already existing\\nmethods.\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 04,2025 at 05:54:21 UTC from IEEE Xplore.  Restrictions apply.'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 4}, page_content='5. REFERENCES\\n[1] Warren J. von Eschenbach, “Transparency and the black\\nbox problem: Why we do not trust ai,” Philosophy &\\nTechnology, vol. 34, no. 4, pp. 1607–1622, 2021.\\n[2] Alejandro Barredo et al. Arrieta, “Explainable ai: Con-\\ncepts, taxonomies, opportunities, and challenges for re-\\nsponsible ai,” Information Fusion, vol. 58, pp. 82–115,\\n2020.\\n[3] Karen et al. Simonyan, “Deep inside convolutional\\nnetworks: Visualizing image classification models and\\nsaliency maps,” arXiv:1312.6034, 2013.\\n[4] Sebastian Bach et al., “On pixel-wise explanations for\\nnon-linear classifier decisions by layer-wise relevance\\npropagation,” PloS one, vol. 10, no. 7, pp. e0130140,\\n2015.\\n[5] Bolei Zhou et al., “Learning deep features for discrimi-\\nnative localization,” in Proceedings of the IEEE confer-\\nence on computer vision and pattern recognition, 2016.\\n[6] Tadas Baltru ˇsaitis et al., “Multimodal machine learning:\\nA survey and taxonomy,” IEEE TPAMI, vol. 41, no. 2,\\npp. 423–443, 2018.\\n[7] Khaled Bayoudh et al., “A survey on deep multimodal\\nlearning for computer vision: advances, trends, appli-\\ncations, and datasets,” The Visual Computer, pp. 1–32,\\n2021.\\n[8] Ross Girshick et al., “Rich feature hierarchies for ac-\\ncurate object detection and semantic segmentation,” in\\nProceedings of the IEEE conference on computer vision\\nand pattern recognition, 2014.\\n[9] Chao Jia et al., “Scaling up visual and vision-language\\nrepresentation learning with noisy text supervision,” in\\nInternational conference on machine learning . PMLR,\\n2021.\\n[10] Stanislaw Antol et al., “Vqa: Visual question answer-\\ning,” in Proceedings of the IEEE international confer-\\nence on computer vision, 2015.\\n[11] Jingyi Zhang et al., “Vision-language models for vi-\\nsion tasks: A survey,”arXiv preprint arXiv:2304.00685,\\n2023.\\n[12] Yuhao Zhang et al., “Contrastive learning of medical\\nvisual representations from paired images and text,” in\\nMachine Learning for Healthcare Conference . PMLR,\\n2022.\\n[13] Shih-Cheng Huang et al., “Gloria: A multi-\\nmodal global-local representation learning framework\\nfor label-efficient medical image recognition,” in ICCV,\\n2021.\\n[14] Zifeng Wang et al., “Medclip: Contrastive learning\\nfrom unpaired medical images and text,” arXiv preprint\\narXiv:2210.10163, 2022.\\n[15] Pierre Chambon et al., “Roentgen: vision-language\\nfoundation model for chest x-ray generation,” arXiv\\npreprint arXiv:2211.12737, 2022.\\n[16] Nida Nasir et al., “Multi-modal image classification of\\ncovid-19 cases using computed tomography and x-ray\\nscans,” Intelligent Systems with Applications , vol. 17,\\npp. 200160, 2023.\\n[17] Alec Radford et al., “Learning transferable visual mod-\\nels from natural language supervision,” in International\\nconference on machine learning. PMLR, 2021.\\n[18] Bas HM van der Velden et al., “Explainable artificial\\nintelligence (xai) in deep learning-based medical image\\nanalysis,” Medical Image Analysis, vol. 79, pp. 102470,\\n2022.\\n[19] Jost Tobias Springenberg et al., “Striving for sim-\\nplicity: The all convolutional net,” arXiv preprint\\narXiv:1412.6806, 2014.\\n[20] Matthew D. Zeiler and Rob Fergus, “Visualizing and\\nunderstanding convolutional networks,” in ECCV 2014.\\n2014, vol. 13, Springer.\\n[21] Mukund Sundararajan, Ankur Taly, and Qiqi Yan, “Ax-\\niomatic attribution for deep networks,” in International\\nconference on machine learning. PMLR, 2017.\\n[22] Ramprasaath R. Selvaraju et al., “Grad-cam: Visual ex-\\nplanations via gradient-based localization,” in ICCV,\\n2017.\\n[23] Ze Liu et al., “Swin transformer: Hierarchical vision\\ntransformer using shifted windows,” in Proceedings of\\nthe IEEE/CVF international conference on computer vi-\\nsion, 2021.\\n[24] Jia Deng et al., “Imagenet: A large-scale hierarchical\\nimage database,” in 2009 IEEE conference on computer\\nvision and pattern recognition. IEEE, 2009.\\n[25] Alistair EW Johnson et al., “Mimic-iii: A freely acces-\\nsible critical care database,” Scientific Data, vol. 3, no.\\n1, pp. 1–9, 2016.'), Document(metadata={'source': 'E:\\\\Shardul\\\\Research-Paper-Summarizer\\\\papers\\\\Envisioning_Medclip_A_Deep_Dive_into_Explainability_for_Medical_Vision-Language_Models.pdf', 'page': 4}, page_content='vision and pattern recognition. IEEE, 2009.\\n[25] Alistair EW Johnson et al., “Mimic-iii: A freely acces-\\nsible critical care database,” Scientific Data, vol. 3, no.\\n1, pp. 1–9, 2016.\\n[26] Alistair EW Johnson et al., “Mimic-cxr, a de-identified\\npublicly available database of chest radiographs with\\nfree-text reports,” Scientific data, vol. 6, no. 1, pp. 317,\\n2019.\\n[27] Adam Paszke et al., “Pytorch: High-performance deep\\nlearning library,”https://pytorch.org, 2019.\\n[28] Narine Kokhlikyan et al., “Captum: A unified model\\ninterpretability library for pytorch,” 2020.\\nAuthorized licensed use limited to: Mukesh Patel School of Technology & Engineering. Downloaded on March 04,2025 at 05:54:21 UTC from IEEE Xplore.  Restrictions apply.')], 'output_text': \"This paper investigates the explainability of MedCLIP, a vision-language model used in medical imaging. It analyzes existing XAI methods, finding them insufficient for VLMs, and proposes a new method that leverages text-image interaction within MedCLIP's embedding space to generate more plausible and informative explanations. The research uses the MIMIC-CXR dataset and finds that standard XAI methods struggle to highlight relevant image areas. The proposed method, which incorporates text prompts, offers improved insights into MedCLIP's decision-making process and its potential for medical applications. The paper also references related research in AI, computer vision, and relevant software/databases.\"}\n"
     ]
    }
   ],
   "source": [
    "summ = summarize(file_path, MODEL)\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper investigates the explainability of MedCLIP, a vision-language model used in medical imaging. It analyzes existing XAI methods, finding them insufficient for VLMs, and proposes a new method that leverages text-image interaction within MedCLIP's embedding space to generate more plausible and informative explanations. The research uses the MIMIC-CXR dataset and finds that standard XAI methods struggle to highlight relevant image areas. The proposed method, which incorporates text prompts, offers improved insights into MedCLIP's decision-making process and its potential for medical applications. The paper also references related research in AI, computer vision, and relevant software/databases.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_summary(file_path, MODEL, custom_prompt):\n",
    "#     loader = PyPDFLoader(file_path)\n",
    "#     docs = loader.load_and_split()\n",
    "#     prompt_template = custom_prompt + \"\"\"\n",
    "#     Answer the following question based only on the provided context, do not use any external information.:\n",
    "\n",
    "#     <context>\n",
    "#     {text}\n",
    "#     </context>\n",
    "    \n",
    "#     SUMMARY:\"\"\"\n",
    "#     PROMPT = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "#     llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)\n",
    "#     chain = load_summarize_chain(llm, chain_type='map_reduce', map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "#     summary = chain({\"input_documents\": docs},return_only_outputs=True)[\"output_text\"]\n",
    "    \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM_PROMPT = \"Write a concise summary of the following paper with this structure: Problem being solved, Approach, Main results and Main Discussion Points. Give output in markdown format.\"\n",
    "CUSTOM_PROMPT = \"Write a concise summary of the methodology used in following paper. Give output in markdown format.\"\n",
    "custom_summaries = custom_summary(file_path, MODEL, custom_prompt=CUSTOM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper's methodology focuses on improving the explainability of the MedCLIP vision-language model. It involves:\n",
      "\n",
      "1.  **Evaluating Existing XAI Methods:** Applying and assessing the performance of established XAI techniques like Gradient backpropagation, Occlusion method, Integrated-Gradients, and Grad-Shapley to generate explainability maps for MedCLIP.\n",
      "2.  **Proposing a Novel Approach:** Developing a new XAI method that leverages the model's internal structure:\n",
      "    *   Applying an XAI method (Mxai) to the image embeddings generated by MedCLIP's vision encoder to create image explainability maps (Fi map).\n",
      "    *   Encoding text prompts using the text encoder to generate text embeddings.\n",
      "    *   Calculating a dot product between the image explainability maps and the text embeddings to highlight image pixels that influence model confidence.\n",
      "3.  **Dataset and Inputs:** Utilizing the MIMIC-CXR dataset with text prompts (sentences) and class labels as text inputs.\n",
      "4.  **Analysis:** Generating feature activation maps to visualize how MedCLIP focuses on specific image regions based on text input, and how its focus changes with different prompts.\n"
     ]
    }
   ],
   "source": [
    "print(custom_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper's methodology focuses on improving the explainability of the MedCLIP vision-language model. It involves:\n",
    "\n",
    "1.  **Evaluating Existing XAI Methods:** Applying and assessing the performance of established XAI techniques like Gradient backpropagation, Occlusion method, Integrated-Gradients, and Grad-Shapley to generate explainability maps for MedCLIP.\n",
    "2.  **Proposing a Novel Approach:** Developing a new XAI method that leverages the model's internal structure:\n",
    "    *   Applying an XAI method (Mxai) to the image embeddings generated by MedCLIP's vision encoder to create image explainability maps (Fi map).\n",
    "    *   Encoding text prompts using the text encoder to generate text embeddings.\n",
    "    *   Calculating a dot product between the image explainability maps and the text embeddings to highlight image pixels that influence model confidence.\n",
    "3.  **Dataset and Inputs:** Utilizing the MIMIC-CXR dataset with text prompts (sentences) and class labels as text inputs.\n",
    "4.  **Analysis:** Generating feature activation maps to visualize how MedCLIP focuses on specific image regions based on text input, and how its focus changes with different prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper's methodology focuses on explaining the Vision-Language Model (VLM) MedCLIP using explainable AI (XAI) techniques. It involves:\n",
    "\n",
    "1.  **Generating Explainability Maps:** Applying XAI methods like Gradient backpropagation (GB), Occlusion, Integrated-Gradients (IG), and Grad-Shapley (GS) to the image embeddings produced by MedCLIP's vision encoder.\n",
    "2.  **Analyzing Text Influence:** Utilizing both class labels and text prompts as input to the model and comparing the resulting activation maps to understand how the model's focus changes based on the text input.\n",
    "3.  **Creating a RAxA weighted average:** Calculating a dot product between the image explainability maps and text embeddings and creating a RAxA weighted average of the maps using the dot product results.\n",
    "4.  **Visualizing and Evaluating:** Highlighting image pixels based on the input text and class labels using chest X-ray images from the MIMIC-CXR dataset to demonstrate the model's ability to link text and image regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Problem being solved:** The paper addresses the lack of interpretability in Vision-Language Models (VLMs), particularly MedCLIP, when applied to medical image analysis. This lack of understanding hinders trust and safe deployment of these models in critical applications.\n",
    "\n",
    "*   **Approach:** The authors propose a novel method to generate feature activation maps for MedCLIP. This method leverages both text prompts (sentences) and class labels as text inputs to analyze how the model focuses on different image regions. The approach aims to overcome the limitations of existing XAI methods by providing more accurate and clinically relevant explanations.\n",
    "\n",
    "*   **Main results:** The proposed method generates focused and accurate activation maps that:\n",
    "    *   Highlight the most important image regions, avoiding false positives, and aligning with clinical diagnostic procedures.\n",
    "    *   Demonstrate how MedCLIP's focus shifts based on the input text prompt, indicating its ability to comprehend text and identify relevant image pixels.\n",
    "    *   Show variations in activation maps across different class labels.\n",
    "\n",
    "*   **Main Discussion Points:**\n",
    "    *   The proposed method overcomes the limitations of conventional XAI methods for VLMs in medical imaging.\n",
    "    *   It effectively explains the functioning of the MedCLIP VLM and the combined effect of image and text inputs.\n",
    "    *   The explainability maps can help understand performance discrepancies in deep learning models.\n",
    "    *   The method is flexible and can be adapted to other VLMs.\n",
    "    *   The importance of making VLMs explainable to increase trustworthiness and facilitate practical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_summary(file_path, MODEL, custom_prompt, vectorDB_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    prompt_template = custom_prompt + \"\"\"\n",
    "    Answer the following question based only on the provided context, do not use any external information.:\n",
    "\n",
    "    <context>\n",
    "    {text}\n",
    "    </context>\n",
    "    \n",
    "    SUMMARY:\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    vectorstore = FAISS.load_local(vectorDB_path, embeddings, allow_dangerous_deserialization=True)  # Load the FAISS index\n",
    "    chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), chain_type='map_reduce', map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "    result = chain({\"question\": query}, return_only_outputs=True)\n",
    "    summary = chain({\"input_documents\": docs},return_only_outputs=True)[\"output_text\"]\n",
    "    # chain = load_summarize_chain(llm, chain_type='map_reduce', map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "    # summary = chain({\"input_documents\": docs},return_only_outputs=True)[\"output_text\"]\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM_PROMPT = \"Write a concise summary of the following paper with this structure: Problem being solved, Approach, Main results and Main Discussion Points. Give output in markdown format.\"\n",
    "CUSTOM_PROMPT = \"Write a detailed summary of the methodology used in following paper. Give output in markdown format.\"\n",
    "vectorDB_path = 'faiss_store'\n",
    "# custom_summaries = custom_summary(file_path, MODEL, custom_prompt=CUSTOM_PROMPT, vectorDB_path=vectorDB_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load_and_split()\n",
    "prompt_template = CUSTOM_PROMPT + \"\"\"\n",
    "Answer the following question based only on the provided context, do not use any external information.:\n",
    "\n",
    "<context>\n",
    "{text}\n",
    "</context>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.5)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "vector_store.save_local(vectorDB_path)\n",
    "vectorstore = FAISS.load_local(vectorDB_path, embeddings, allow_dangerous_deserialization=True)  # Load the FAISS index\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
    "result = chain({\"question\": prompt_template}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the methodology used in the paper, formatted in Markdown:\n",
      "\n",
      "**Methodology Summary**\n",
      "\n",
      "The paper focuses on explaining the inner workings of the Vision-Language Model (VLM) MedCLIP, specifically for chest X-ray (CXR) classification, using Explainable AI (XAI) methods. The core approach involves applying these XAI techniques to the embedding space of the VLM, rather than the final output. The methodology is designed to be generalizable to other VLMs.\n",
      "\n",
      "**Key Steps and Components:**\n",
      "\n",
      "1.  **XAI Methods:** The paper utilizes four XAI methods:\n",
      "    *   Gradient Backpropagation (GB)\n",
      "    *   Occlusion Method\n",
      "    *   Integrated-Gradients (IG)\n",
      "    *   Grad-Shapley (GS)\n",
      "\n",
      "2.  **Image Embedding Explainability:** An XAI method (Mxai) is applied to the image embeddings generated by the vision-encoder (Venc) of the MedCLIP model to generate an explainability map (Fi map ∈ RAxA) for each embedding dimension (i).\n",
      "\n",
      "    *   Formula: Fi map = Mxai(model = Venc, target= i), where i is the index of the image embedding Ip ∈ R1x512.\n",
      "\n",
      "3.  **Text Embedding Generation:** Text input (Xtxt) is encoded through the text encoder (Tend) to generate text embeddings (Tp ∈ R1xM). These embeddings are then scaled by the learned temperature parameter of the VLM.\n",
      "\n",
      "4.  **Application to MedCLIP:** The XAI methods are applied to MedCLIP to generate explainability maps for a given image.\n",
      "\n",
      "5.  **Input:** The method uses both images and text prompts (sentences describing various lesions) and class labels.\n",
      "\n",
      "6.  **Output:** The method generates feature activation maps to explain the MedCLIP model, highlighting the most important image pixels. The highlighted pixel locations align with clinical diagnostic procedures. The explainability maps visualize the difference in activation maps for different text input forms. The approach avoids false positives. The method uses image and text embedding fusion approach.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary of the methodology used in the paper, formatted in Markdown:\n",
    "\n",
    "**Methodology Summary**\n",
    "\n",
    "The paper focuses on explaining the inner workings of the Vision-Language Model (VLM) MedCLIP, specifically for chest X-ray (CXR) classification, using Explainable AI (XAI) methods. The core approach involves applying these XAI techniques to the embedding space of the VLM, rather than the final output. The methodology is designed to be generalizable to other VLMs.\n",
    "\n",
    "**Key Steps and Components:**\n",
    "\n",
    "1.  **XAI Methods:** The paper utilizes four XAI methods:\n",
    "    *   Gradient Backpropagation (GB)\n",
    "    *   Occlusion Method\n",
    "    *   Integrated-Gradients (IG)\n",
    "    *   Grad-Shapley (GS)\n",
    "\n",
    "2.  **Image Embedding Explainability:** An XAI method (Mxai) is applied to the image embeddings generated by the vision-encoder (Venc) of the MedCLIP model to generate an explainability map (Fi map ∈ RAxA) for each embedding dimension (i).\n",
    "\n",
    "    *   Formula: Fi map = Mxai(model = Venc, target= i), where i is the index of the image embedding Ip ∈ R1x512.\n",
    "\n",
    "3.  **Text Embedding Generation:** Text input (Xtxt) is encoded through the text encoder (Tend) to generate text embeddings (Tp ∈ R1xM). These embeddings are then scaled by the learned temperature parameter of the VLM.\n",
    "\n",
    "4.  **Application to MedCLIP:** The XAI methods are applied to MedCLIP to generate explainability maps for a given image.\n",
    "\n",
    "5.  **Input:** The method uses both images and text prompts (sentences describing various lesions) and class labels.\n",
    "\n",
    "6.  **Output:** The method generates feature activation maps to explain the MedCLIP model, highlighting the most important image pixels. The highlighted pixel locations align with clinical diagnostic procedures. The explainability maps visualize the difference in activation maps for different text input forms. The approach avoids false positives. The method uses image and text embedding fusion approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
